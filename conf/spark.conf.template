##############################################################################################
# Spark Config File, These are default configuration settings and can be adapted as required.#
# See spark documentation page 'Configuration' for details.                                  #
# Current location: http://spark.incubator.apache.org/docs/latest/configuration.html         #
# This list of config properties is not exhaustive and does not include optional properties. #
##############################################################################################
{

    worker {
        ui {
            port = 8081
        }
    }

    spark {
        test {
            # Used for testing only.
            disableBlockManagerHeartBeat = false
        }
        serializer = "org.apache.spark.serializer.JavaSerializer"
        broadcast {
            compress = true
            blockSize = 4096
            factory = "org.apache.spark.broadcast.HttpBroadcastFactory"
        }
        deploy {
            zookeeper {
                dir = "/spark"
                url = ""
            }
            retainedApplications = 200
            spreadOut = true
            recoveryMode = "NONE"
            recoveryDirectory = ""
        }
        simr {
            executor {
                cores = 1
            }
        }
        cores {
            max = 2147483647
        }
        buffer {
            size = 65536
        }
        diskStore {
            subDirectories = 64
        }
        mesos {
            extra {
                cores = 0
            }
            coarse = false
        }
        speculation = false
        worker {
            timeout = 60
        }
        dead {
            worker {
                persistence = 15
            }
        }
        rdd {
            compress = false
        }
        streaming {
            clock = "org.apache.spark.streaming.util.SystemClock"
            manualClock {
                jump = 0
            }
            concurrentJobs = 1
            blockInterval = 200
        }
        ui {
            port = 4040
            retainedStages= 1000
        }
        reducer {
            maxMbInFlight = 48
        }
        resultGetter {
            threads = 4
        }
        scheduler {
            mode = "FIFO"
            revive {
                interval = 1000
            }
        }
        task {
            cpus = 1
            maxFailures = 4
        }
        starvation {
            timeout = 15000
        }
        executor {
            memory = 512
        }
        logging {
            exceptionPrintInterval = 10000
        }
        logConf = false
        io {
            compression {
                snappy {
                    block {
                        size = 32768
                    }
                }
                codec = "org.apache.spark.io.LZFCompressionCodec"
            }
        }

        akka {
            failureDetector {
                threshold = 300
            }
            num {
                retries = 3
            }
            logLifecycleEvents = false
            threads = 4
            askTimeout = 10
            heartbeat {
                interval = 1000
                pauses = 600
            }
            frameSize = 10
            batchSize = 15
            retry {
                wait = 3000
            }
            timeout = 60
        }

        shuffle {
            compress = true
            sender {
                port = 0
            }
            fetcher = "org.apache.spark.BlockStoreShuffleFetcher"
            copier {
                threads = 6
            }
            sync = false
            netty {
                connect {
                    timeout = 60000
                }
            }
            file {
                buffer {
                    kb = 100
                }
            }
            use {
                netty = false
            }
            consolidateFiles = true
        }
        cleaner {
            ttl = 3600
        }
        locality {
            wait {
                process = 3000
                node = 3000
                rack = 3000
            }
        }
        closure {
            serializer = "org.apache.spark.serializer.JavaSerializer"
        }

        kryo {
            referenceTracking = true
        }

        core {
            connection {
                connect {
                    threads {
                        min = 1
                        max = 8
                        keepalive = 60
                    }
                }

                handler {
                    threads {
                        min = 20
                        max = 60
                        keepalive = 60
                    }
                }

                io {
                    threads {
                      min = 4
                      max = 32
                      keepalive = 60
                    }
                }
            }
        }

        kryoserializer {
            buffer {
                mb = 2
            }
        }

        local {
            dir = "/tmp"
        }

        yarn {
            submit {
                file {
                    replication = 3
                }
            }

            scheduler {
                heartbeat {
                    interval-ms = 5000
                }
            }

            applicationMaster {
                waitTries = 10
            }

            user {
                classpath {
                    first = false
                }
            }

            preserve {
                staging {
                    files = false
                }
            }
        }

        storage {
            blockManagerTimeoutIntervalMs = 60000
            memoryFraction = 0.66
            blockmanager {
                maxmem = 339585269
            }
            blockManagerSlaveTimeoutMs = 180000
        }
    }
}
